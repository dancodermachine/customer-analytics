{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Convertion Prediction\n",
    "\n",
    "Goal: How likely it is for an existing customer to convert again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "\n",
    "seed=101096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Book length (mins)_overall</th>\n",
       "      <th>Book length (mins)_avg</th>\n",
       "      <th>Price_overall</th>\n",
       "      <th>Price_avg</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review 10/10</th>\n",
       "      <th>Minutes listened</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Support Requests</th>\n",
       "      <th>Las visited minus Purchase date</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>873</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>2808</td>\n",
       "      <td>6.66</td>\n",
       "      <td>13.33</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>705</td>\n",
       "      <td>324.0</td>\n",
       "      <td>324</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>391</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620</td>\n",
       "      <td>15.31</td>\n",
       "      <td>15.31</td>\n",
       "      <td>0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1296</td>\n",
       "      <td>7.11</td>\n",
       "      <td>21.33</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Book length (mins)_overall  Book length (mins)_avg  Price_overall  \\\n",
       "0  873                      2160.0                    2160          10.13   \n",
       "1  611                      1404.0                    2808           6.66   \n",
       "2  705                       324.0                     324          10.13   \n",
       "3  391                      1620.0                    1620          15.31   \n",
       "4  819                       432.0                    1296           7.11   \n",
       "\n",
       "   Price_avg  Review  Review 10/10  Minutes listened  Completion  \\\n",
       "0      10.13       0          8.91               0.0         0.0   \n",
       "1      13.33       1          6.50               0.0         0.0   \n",
       "2      10.13       1          9.00               0.0         0.0   \n",
       "3      15.31       0          9.00               0.0         0.0   \n",
       "4      21.33       1          9.00               0.0         0.0   \n",
       "\n",
       "   Support Requests  Las visited minus Purchase date  Targets  \n",
       "0                 0                                0        1  \n",
       "1                 0                              182        1  \n",
       "2                 1                              334        1  \n",
       "3                 0                              183        1  \n",
       "4                 0                                0        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['ID',\n",
    "                'Book length (mins)_overall',\n",
    "                'Book length (mins)_avg',\n",
    "                'Price_overall',\n",
    "                'Price_avg',\n",
    "                'Review',\n",
    "                'Review 10/10',\n",
    "                'Minutes listened',\n",
    "                'Completion',\n",
    "                'Support Requests',\n",
    "                'Las visited minus Purchase date',\n",
    "                'Targets']\n",
    "\n",
    "df = pd.read_csv('Data/audiobooks_data.csv', names=column_names, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14084 entries, 0 to 14083\n",
      "Data columns (total 12 columns):\n",
      "ID                                 14084 non-null int64\n",
      "Book length (mins)_overall         14084 non-null float64\n",
      "Book length (mins)_avg             14084 non-null int64\n",
      "Price_overall                      14084 non-null float64\n",
      "Price_avg                          14084 non-null float64\n",
      "Review                             14084 non-null int64\n",
      "Review 10/10                       14084 non-null float64\n",
      "Minutes listened                   14084 non-null float64\n",
      "Completion                         14084 non-null float64\n",
      "Support Requests                   14084 non-null int64\n",
      "Las visited minus Purchase date    14084 non-null int64\n",
      "Targets                            14084 non-null int64\n",
      "dtypes: float64(6), int64(6)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to match the number of 1s to the number of 0s in order to balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2237 11847\n"
     ]
    }
   ],
   "source": [
    "one_count = df[df['Targets']==1].shape[0]\n",
    "zero_count = df[df['Targets']==0].shape[0]\n",
    "print(one_count, zero_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_target = df[df['Targets']==0].sample(one_count, random_state=seed)\n",
    "one_target = df[df['Targets']==1]\n",
    "new_df = pd.concat([zero_target, one_target], axis=0)\n",
    "new_df = shuffle(new_df, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2237 2237\n"
     ]
    }
   ],
   "source": [
    "new_one_count = new_df[new_df['Targets']==1].shape[0]\n",
    "new_zero_count = new_df[new_df['Targets']==0].shape[0]\n",
    "print(new_one_count, new_zero_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = ['Book length (mins)_overall',\n",
    "             'Book length (mins)_avg',\n",
    "             'Price_overall',\n",
    "             'Price_avg',\n",
    "             'Review',\n",
    "             'Review 10/10',\n",
    "             'Minutes listened',\n",
    "             'Completion',\n",
    "             'Support Requests',\n",
    "             'Las visited minus Purchase date']\n",
    "X = new_df[x_columns]\n",
    "y = new_df['Targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_valid_scaler = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [*zip(X_train_scaler.astype(np.float32), y_train.values)]\n",
    "valid = [*zip(X_valid_scaler.astype(np.float32), y_valid.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.fc4 = nn.Linear(50, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened      \n",
    "        x = self.dropout(F.elu(self.fc1(x)))\n",
    "        x = self.dropout(F.elu(self.fc2(x)))\n",
    "        x = self.dropout(F.elu(self.fc3(x)))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50..  Training Loss: 0.030..  Valid Loss: 0.027..  Valid Accuracy: 0.753\n",
      "Epoch: 2/50..  Training Loss: 0.026..  Valid Loss: 0.026..  Valid Accuracy: 0.753\n",
      "Epoch: 3/50..  Training Loss: 0.026..  Valid Loss: 0.026..  Valid Accuracy: 0.745\n",
      "Epoch: 4/50..  Training Loss: 0.025..  Valid Loss: 0.025..  Valid Accuracy: 0.784\n",
      "Epoch: 5/50..  Training Loss: 0.025..  Valid Loss: 0.025..  Valid Accuracy: 0.775\n",
      "Epoch: 6/50..  Training Loss: 0.024..  Valid Loss: 0.025..  Valid Accuracy: 0.778\n",
      "Epoch: 7/50..  Training Loss: 0.024..  Valid Loss: 0.025..  Valid Accuracy: 0.777\n",
      "Epoch: 8/50..  Training Loss: 0.024..  Valid Loss: 0.024..  Valid Accuracy: 0.800\n",
      "Epoch: 9/50..  Training Loss: 0.023..  Valid Loss: 0.024..  Valid Accuracy: 0.800\n",
      "Epoch: 10/50..  Training Loss: 0.023..  Valid Loss: 0.024..  Valid Accuracy: 0.803\n",
      "Epoch: 11/50..  Training Loss: 0.023..  Valid Loss: 0.023..  Valid Accuracy: 0.792\n",
      "Epoch: 12/50..  Training Loss: 0.023..  Valid Loss: 0.024..  Valid Accuracy: 0.807\n",
      "Epoch: 13/50..  Training Loss: 0.023..  Valid Loss: 0.023..  Valid Accuracy: 0.800\n",
      "Epoch: 14/50..  Training Loss: 0.023..  Valid Loss: 0.023..  Valid Accuracy: 0.806\n",
      "Epoch: 15/50..  Training Loss: 0.023..  Valid Loss: 0.023..  Valid Accuracy: 0.803\n",
      "Epoch: 16/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.791\n",
      "Epoch: 17/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.808\n",
      "Epoch: 18/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.796\n",
      "Epoch: 19/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.813\n",
      "Epoch: 20/50..  Training Loss: 0.022..  Valid Loss: 0.024..  Valid Accuracy: 0.797\n",
      "Epoch: 21/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.823\n",
      "Epoch: 22/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.813\n",
      "Epoch: 23/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.811\n",
      "Epoch: 24/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.819\n",
      "Epoch: 25/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.812\n",
      "Epoch: 26/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.823\n",
      "Epoch: 27/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.821\n",
      "Epoch: 28/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.813\n",
      "Epoch: 29/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.818\n",
      "Epoch: 30/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.816\n",
      "Epoch: 31/50..  Training Loss: 0.022..  Valid Loss: 0.023..  Valid Accuracy: 0.819\n",
      "Epoch: 32/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.817\n",
      "Epoch: 33/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.829\n",
      "Epoch: 34/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.813\n",
      "Epoch: 35/50..  Training Loss: 0.022..  Valid Loss: 0.022..  Valid Accuracy: 0.818\n",
      "Epoch: 36/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.823\n",
      "Epoch: 37/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.820\n",
      "Epoch: 38/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.820\n",
      "Epoch: 39/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.815\n",
      "Epoch: 40/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.819\n",
      "Epoch: 41/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.822\n",
      "Epoch: 42/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.817\n",
      "Epoch: 43/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.808\n",
      "Epoch: 44/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.812\n",
      "Epoch: 45/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.825\n",
      "Epoch: 46/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.821\n",
      "Epoch: 47/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.827\n",
      "Epoch: 48/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.828\n",
      "Epoch: 49/50..  Training Loss: 0.021..  Valid Loss: 0.022..  Valid Accuracy: 0.830\n",
      "Epoch: 50/50..  Training Loss: 0.021..  Valid Loss: 0.021..  Valid Accuracy: 0.819\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = [], []\n",
    "for e in range(epochs):\n",
    "    tot_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        tot_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        tot_valid_loss = 0\n",
    "        valid_correct = 0  # Number of correct predictions on the valid set\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                log_ps = model(inputs)\n",
    "                loss = criterion(log_ps, labels)\n",
    "                tot_valid_loss += loss.item()\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                valid_correct += equals.sum().item()\n",
    "\n",
    "        # Get mean loss to enable comparison between train and valid sets\n",
    "        train_loss = tot_train_loss / len(train_loader.dataset)\n",
    "        valid_loss = tot_valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "        # At completion of epoch\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "              \"Valid Loss: {:.3f}.. \".format(valid_loss),\n",
    "              \"Valid Accuracy: {:.3f}\".format(valid_correct / len(valid_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
